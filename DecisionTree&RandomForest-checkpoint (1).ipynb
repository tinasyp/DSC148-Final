{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6G8uG6sm-gGj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0VgqMGh9-gGm"
   },
   "source": [
    "https://en.wikipedia.org/wiki/Decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xgiZBDKB-gGn"
   },
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9G43Tqa-gGn"
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    \n",
    "    Decision Tree Classifier\n",
    "    \n",
    "    Attributes:\n",
    "        root: Root Node of the tree.\n",
    "        max_depth: Max depth allowed for the tree\n",
    "        size_allowed : Min_size split, smallest size allowed for split \n",
    "        n_features: Number of features to use during building the tree.(Random Forest)\n",
    "        n_split:  Number of split for each feature. (Random Forest)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_depth = 1000, size_allowed = 1, n_features = None, n_split = None):\n",
    "        \"\"\"\n",
    "        \n",
    "            Initializations for class attributes.\n",
    "            \n",
    "            TODO: 1. Modify the initialization of the attributes of the Decision Tree classifier\n",
    "        \"\"\"\n",
    "        \n",
    "        self.root = None             \n",
    "        self.max_depth = max_depth        \n",
    "        self.size_allowed = size_allowed      \n",
    "        self.n_features = n_features        \n",
    "        self.n_split = n_split           \n",
    "    \n",
    "    \n",
    "    class Node():\n",
    "        \"\"\"\n",
    "            Node Class for the building the tree.\n",
    "\n",
    "            Attribute: \n",
    "                threshold: The threshold like if x1 < threshold, for spliting.\n",
    "                feature: The index of feature on this current node.\n",
    "                left: Pointer to the node on the left.\n",
    "                right: Pointer to the node on the right.\n",
    "                pure: Bool, describe if this node is pure.\n",
    "                predict: Class, indicate what the most common Y on this node.\n",
    "\n",
    "        \"\"\"\n",
    "        def __init__(self, threshold = None, feature = None):\n",
    "            \"\"\"\n",
    "            \n",
    "                Initializations for class attributes.\n",
    "                \n",
    "                TODO: 2. Modify the initialization of the attributes of the Node. (Initialize threshold and feature)\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            self.threshold = threshold   \n",
    "            self.feature = feature    \n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.pure = None\n",
    "            self.depth = None\n",
    "            self.predict = None\n",
    "    \n",
    "    \n",
    "    def entropy(self, lst):\n",
    "        \"\"\"\n",
    "            Function Calculate the entropy given lst.\n",
    "            \n",
    "            Attributes: \n",
    "                entro: variable store entropy for each step.\n",
    "                classes: all possible classes. (without repeating terms)\n",
    "                counts: counts of each possible classes.\n",
    "                total_counts: number of instances in this lst.\n",
    "                \n",
    "            lst is vector of labels.\n",
    "                \n",
    "            \n",
    "            \n",
    "            TODO: 3. Intilize attributes.\n",
    "                  4. Modify and add some codes to the following for-loop\n",
    "                     to compute the correct entropy. \n",
    "                     (make sure count of corresponding label is not 0, think about why we need to do that.)\n",
    "        \"\"\"\n",
    "        \n",
    "        entro = 0  \n",
    "        classes = []  \n",
    "        counts = [] \n",
    "        total_counts = len(lst)   \n",
    "        for label in lst:       \n",
    "            if label not in classes and label is not None:\n",
    "                classes.append(label)\n",
    "                counts.append(1)\n",
    "            elif label in classes:\n",
    "                idx = classes.index(label)\n",
    "                counts[idx]+=1\n",
    "        for c in counts:\n",
    "            prob = c/total_counts\n",
    "            entro -= prob * np.log(prob) if c > 0 else 0\n",
    "        return entro\n",
    "\n",
    "    def information_gain(self, lst, values, threshold):\n",
    "        \"\"\"\n",
    "        \n",
    "            Function Calculate the information gain, by using entropy function.\n",
    "            \n",
    "            lst is vector of labels.\n",
    "            values is vector of values for individule feature.\n",
    "            threshold is the split threshold we want to use for calculating the entropy.\n",
    "            \n",
    "            \n",
    "            TODO:\n",
    "                5. Modify the following variable to calculate the P(left node), P(right node), \n",
    "                   Conditional Entropy(left node) and Conditional Entropy(right node)\n",
    "                6. Return information gain.\n",
    "                \n",
    "                \n",
    "        \"\"\"\n",
    "        \n",
    "        left_lst = []\n",
    "        right_lst = []\n",
    "        for i in range(len(values)):\n",
    "            if values[i] <= threshold:\n",
    "                left_lst.append(lst[i])\n",
    "            else:\n",
    "                right_lst.append(lst[i])\n",
    "            \n",
    "        left_prop = len(left_lst)/len(lst)       \n",
    "        right_prop = len(right_lst)/len(lst)    \n",
    "\n",
    "        left_entropy = self.entropy(left_lst)   \n",
    "        right_entropy = self.entropy(right_lst)\n",
    "        \n",
    "        parent_entro = self.entropy(lst)\n",
    "        conditional = left_prop*left_entropy + right_prop*right_entropy\n",
    "        \n",
    "        info_gain = parent_entro - conditional\n",
    "        \n",
    "        return info_gain  \n",
    "    \n",
    "    def find_rules(self, data):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "            Helper function to find the split rules.\n",
    "            \n",
    "            data is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "            \n",
    "            TODO: 7. Modify the following for loop, which loop through each column(feature).\n",
    "                     Find the unique value of each feature, and find the mid point of each adjacent value.\n",
    "                  8. Store them in a list, return that list.\n",
    "            \n",
    "        \"\"\"\n",
    "        n,m = data.shape       \n",
    "        rules = []        \n",
    "        for i in range(m):          \n",
    "            unique_value = np.unique(data[:,i])      \n",
    "            diff  = np.diff(unique_value)   \n",
    "            mid = unique_value[:-1]+diff/2\n",
    "            rules.append(mid)             \n",
    "        return rules\n",
    "    \n",
    "    def next_split(self, data, label):\n",
    "        \"\"\"\n",
    "            Helper function to find the split with most information gain, by using find_rules, and information gain.\n",
    "            \n",
    "            data is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "            \n",
    "            label contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "            \n",
    "            TODO: 9. Use find_rules to initialize rules variable\n",
    "                  10. Initialize max_info to some negative number.\n",
    "        \"\"\"\n",
    "        \n",
    "        rules = self.find_rules(data)             \n",
    "        max_info = -1          \n",
    "        num_col = None          \n",
    "        threshold = None       \n",
    "        entropy_y = self.entropy(label)      \n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "            TODO: 11. Check Number of features to use, None means all featurs. (Decision Tree always use all feature)\n",
    "                      If n_features is a int, use n_features of features by random choice. \n",
    "                      If n_features == 'sqrt', use sqrt(Total Number of Features ) by random choice.\n",
    "                      \n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "#         if True :\n",
    "#             index_col = []\n",
    "#         else:\n",
    "#             if True: \n",
    "#                 num_index = 1  \n",
    "#             elif True:\n",
    "#                 num_index = 1  \n",
    "#             np.random.seed()  \n",
    "#             index_col = np.random.choice(data.shape[1], num_index, replace = False)\n",
    "        if self.n_features is None:\n",
    "            index_col = np.arange(data.shape[1])\n",
    "        elif self.n_features == 'sqrt':\n",
    "            self.n_features = int(np.sqrt(data.shape[1]))\n",
    "            index_col = np.random.choice(data.shape[1], self.n_features, replace=False)\n",
    "        else:\n",
    "            index_col = np.random.choice(data.shape[1], self.n_features, replace=False)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "            TODO: 12. Do the similar selection we did for features, n_split take in None or int or 'sqrt'.\n",
    "                  13. For all selected feature and corresponding rules, we check it's information gain. \n",
    "                  \n",
    "        \"\"\"\n",
    "#         for i in index_col:\n",
    "#             count_temp_rules = 1\n",
    "            \n",
    "#             if True :\n",
    "#                 index_rules = []\n",
    "#             else:\n",
    "                \n",
    "#                 if True:\n",
    "#                     num_rules = 1   \n",
    "#                 elif True:\n",
    "#                     num_rules = 1  \n",
    "#                 np.random.seed()\n",
    "#                 index_rules = np.random.choice(count_temp_rules, num_rules, replace = False)\n",
    "        for i in index_col:\n",
    "            count_temp_rules = len(rules[i])\n",
    "            if self.n_split is None:\n",
    "                index_rules = np.arange(count_temp_rules)\n",
    "            elif self.n_split == 'sqrt':\n",
    "                self.n_splits = int(np.sqrt(count_temp_rules))\n",
    "                index_rules = np.random.choice(count_temp_rules, self.n_splits, replace=False)\n",
    "            else:\n",
    "                index_rules = np.random.choice(count_temp_rules, self.n_split, replace=False)\n",
    "            \n",
    "            for j in index_rules:\n",
    "                info = self.information_gain(label,data[:,i],rules[i][j])     \n",
    "                if info > max_info:  \n",
    "                    max_info = info\n",
    "                    num_col = i\n",
    "                    threshold = rules[i][j]\n",
    "        return threshold, num_col\n",
    "        \n",
    "    def build_tree(self, X, y, depth):\n",
    "        \n",
    "            \"\"\"\n",
    "                Helper function for building the tree.\n",
    "                \n",
    "                TODO: 14. First build the root node.\n",
    "            \"\"\"\n",
    "            \n",
    "            \n",
    "            first_threshold, first_feature = self.next_split(X,y) \n",
    "            current = self.Node(first_threshold, first_feature)  \n",
    "            \n",
    "            \"\"\"\n",
    "                TODO: 15. Base Case 1: Check if we pass the max_depth, check if the first_feature is None, min split size.\n",
    "                          If some of those condition met, change current to pure, and set predict to the most popular label\n",
    "                          and return current\n",
    "                          \n",
    "                \n",
    "            \"\"\"\n",
    "            if (depth >= self.max_depth) or (first_feature is None) or (len(y) < self.size_allowed) :\n",
    "                count = np.bincount(y.astype(int))\n",
    "                current.predict = np.argmax(count)\n",
    "                current.pure = True\n",
    "                return current\n",
    "            \n",
    "            \"\"\"\n",
    "               Base Case 2: Check if there is only 1 label in this node, change current to pure, and set predict to the label\n",
    "            \"\"\"\n",
    "            \n",
    "            if len(np.unique(y)) == 1:\n",
    "                current.predict = y[0]\n",
    "                current.pure = True\n",
    "                return current\n",
    "            \n",
    "            \"\"\"\n",
    "                TODO: 16. Find the left node index with feature i <= threshold  Right with feature i > threshold.\n",
    "            \"\"\"\n",
    "            \n",
    "\n",
    "            \n",
    "            left_index =  np.where(X[:,first_feature] <= first_threshold)[0]\n",
    "            right_index = np.where(X[:,first_feature] > first_threshold)[0]\n",
    "            \n",
    "            \"\"\"\n",
    "                TODO: 17. Base Case 3: If we either side is empty, change current to pure, and set predict to the label\n",
    "            \"\"\"\n",
    "            if (len(left_index) == 0) or (len(right_index) == 0):\n",
    "                count = np.bincount(y.astype(int))\n",
    "                current.predict = np.argmax(count)\n",
    "                current.pure = True \n",
    "                return current\n",
    "            \n",
    "            \n",
    "            left_X, left_y = X[left_index,:], y[left_index]\n",
    "            current.left = self.build_tree(left_X, left_y, depth + 1)\n",
    "                \n",
    "            right_X, right_y = X[right_index,:], y[right_index]\n",
    "            current.right = self.build_tree(right_X, right_y, depth + 1)\n",
    "            \n",
    "            return current\n",
    "    \n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        \"\"\"\n",
    "            The fit function fits the Decision Tree model based on the training data. \n",
    "            \n",
    "            X_train is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "\n",
    "            y_train contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "        \"\"\"\n",
    "        self.root = self.build_tree(X, y, 1)\n",
    "        \n",
    "\n",
    "        self.for_running = y[0]\n",
    "        return self\n",
    "            \n",
    "    def ind_predict(self, inp):\n",
    "        \"\"\"\n",
    "            Predict the most likely class label of one test instance based on its feature vector x.\n",
    "            \n",
    "            TODO: 18. Modify the following while loop to get the prediction.\n",
    "                      Stop condition we are at a node is pure.\n",
    "                      Trace with the threshold and feature.\n",
    "                19. Change return self.for_running to appropiate value.\n",
    "        \"\"\"\n",
    "        cur = self.root  \n",
    "        while not cur.pure:  \n",
    "            \n",
    "            feature = cur.feature  \n",
    "            threshold = cur.threshold \n",
    "            \n",
    "            if inp[feature] <= threshold:  \n",
    "                cur = cur.left\n",
    "            else:\n",
    "                cur = cur.right\n",
    "        return cur.predict\n",
    "    \n",
    "    def predict(self, inp):\n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Return the predictions of all instances in a list.\n",
    "            \n",
    "            TODO: 20. Revise the following for-loop to call ind_predict to get predictions.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = []\n",
    "        for i in range(inp.shape[0]):\n",
    "            result.append(self.ind_predict(inp[i]))\n",
    "        return result\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "boiJ3622-gGp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2uPtiFV-gGr"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            7.4              0.70         0.00             1.9      0.076   \n",
       "1            7.8              0.88         0.00             2.6      0.098   \n",
       "2            7.8              0.76         0.04             2.3      0.092   \n",
       "3           11.2              0.28         0.56             1.9      0.075   \n",
       "4            7.4              0.70         0.00             1.9      0.076   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
       "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
       "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
       "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
       "\n",
       "   alcohol  quality  \n",
       "0      9.4        5  \n",
       "1      9.8        5  \n",
       "2      9.8        5  \n",
       "3      9.8        6  \n",
       "4      9.4        5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url_Wine = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "wine = pd.read_csv(url_Wine, delimiter=';')\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FC1zQF20-gGt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "umUfc-Kj-gGv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.4  ,  0.7  ,  0.   , ...,  3.51 ,  0.56 ,  9.4  ],\n",
       "       [ 7.8  ,  0.88 ,  0.   , ...,  3.2  ,  0.68 ,  9.8  ],\n",
       "       [ 7.8  ,  0.76 ,  0.04 , ...,  3.26 ,  0.65 ,  9.8  ],\n",
       "       ...,\n",
       "       [ 6.3  ,  0.51 ,  0.13 , ...,  3.42 ,  0.75 , 11.   ],\n",
       "       [ 5.9  ,  0.645,  0.12 , ...,  3.57 ,  0.71 , 10.2  ],\n",
       "       [ 6.   ,  0.31 ,  0.47 , ...,  3.39 ,  0.66 , 11.   ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(wine)[:, :-1]\n",
    "y = np.array(wine)[:, -1]\n",
    "y = np.array(y.flatten())\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g8lOQ1vd-gGx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5p2aQgpE-gGz"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eACyWCM9-gG1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMzJB5Ot-gG2",
    "outputId": "c939a2b6-1f8d-4e5b-f013-e3afb125ad9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DecisionTree at 0x7fdfc30c8bb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTree()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ul2dkqPh-gG5"
   },
   "source": [
    "### Train Error should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mrinqIMZ-gG6",
    "outputId": "5d0a4110-6d35-4700-b6dc-e4f5bf928f65"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(X_train)\n",
    "(pred == y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMu0yRH1-gG8"
   },
   "source": [
    "### Test Error should be around 0.62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mw0_bBUU-gG8"
   },
   "outputs": [],
   "source": [
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hIVcRBSC-gG-",
    "outputId": "18a7d8fb-17c9-49f1-a0a2-a0cdc9aa1c01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.621875"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EuWhhXnn-gG_"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetes_binary_health_indicators_BRFSS2015.csv')\n",
    "df_clean = df.drop(columns = ['Fruits','AnyHealthcare','Sex','NoDocbcCost'])\n",
    "X = np.array(df.iloc[:, 1:])\n",
    "y = np.array(df['Diabetes_binary'])\n",
    "y = np.array(y.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DecisionTree at 0x7fdfc3213be0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,random_state = 0)\n",
    "clf = DecisionTree()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8002404604225797"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict(X_test)\n",
    "# (pred == y_train).mean()\n",
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tSgiIwTc-gHB"
   },
   "source": [
    "https://en.wikipedia.org/wiki/Random_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f0gYN1_H-gHB"
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    RandomForest Classifier\n",
    "    \n",
    "    Attributes:\n",
    "        n_trees: Number of trees. \n",
    "        trees: List store each individule tree\n",
    "        n_features: Number of features to use during building each individule tree.\n",
    "        n_split: Number of split for each feature.\n",
    "        max_depth: Max depth allowed for the tree\n",
    "        size_allowed : Min_size split, smallest size allowed for split \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,n_trees = 10, n_features = 'sqrt', n_split = 'sqrt', max_depth = 100, size_allowed = 1):\n",
    "        \n",
    "        \"\"\"\n",
    "            Initilize all Attributes.\n",
    "            \n",
    "            TODO: 1. Intialize n_trees, n_features, n_split, max_depth, size_allowed.\n",
    "        \"\"\"\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = []\n",
    "        self.n_features = n_features\n",
    "        self.n_split = n_split\n",
    "        self.max_depth = max_depth\n",
    "        self.size_allowed = size_allowed\n",
    "        \n",
    "        \n",
    "    def fit(self, X,y):\n",
    "        \n",
    "        \"\"\"\n",
    "            The fit function fits the Random Forest model based on the training data. \n",
    "            \n",
    "            X_train is a matrix or 2-D numpy array, represnting training instances. \n",
    "            Each training instance is a feature vector. \n",
    "            \n",
    "            y_train contains the corresponding labels. There might be multiple (i.e., > 2) classes.\n",
    "            \n",
    "        \n",
    "            TODO: 2. Modify the following for loop to create n_trees tress. by calling DecisionTree we created.\n",
    "                     Pass in all the attributes.\n",
    "        \"\"\"\n",
    "#         self.for_running = y[4]\n",
    "        \n",
    "        for i in range(self.n_trees):\n",
    "            np.random.seed()\n",
    "            temp_clf = DecisionTree(self.max_depth, self.size_allowed, self.n_features, self.n_split)\n",
    "            temp_clf.fit(X, y)\n",
    "            self.trees.append(temp_clf)\n",
    "        return self\n",
    "            \n",
    "    def ind_predict(self, inp):\n",
    "        \n",
    "        \"\"\"\n",
    "            Predict the most likely class label of one test instance based on its feature vector x.\n",
    "        \n",
    "            TODO: 4. Modify the following code to predict using each Decision Tree.\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for i in self.trees:\n",
    "            result.append(i.ind_predict(inp))\n",
    "            \n",
    "                \n",
    "        \"\"\"\n",
    "            TODO: 5. Modify the following code to find the correct prediction use majority rule.\n",
    "                     If there is a tie, use random choice to select one of them.\n",
    "        \"\"\"\n",
    "#         labels, counts = [result[0]],[len(result)]\n",
    "#         return labels\n",
    "        counts = {}\n",
    "        for label in result:\n",
    "            if label not in counts:\n",
    "                counts[label] = 0\n",
    "            counts[label] += 1\n",
    "        sorted_labels = sorted(counts.keys(), key=lambda x: counts[x], reverse=True)\n",
    "        \n",
    "        # Check if there is a tie\n",
    "        if len(sorted_labels) > 1 and counts[sorted_labels[0]] == counts[sorted_labels[1]]:\n",
    "            # Randomly select one of the tied labels\n",
    "            label = np.random.choice(sorted_labels[:2])\n",
    "        else:\n",
    "            label = sorted_labels[0]\n",
    "            \n",
    "        return label\n",
    "    \n",
    "    def predict_all(self, inp):\n",
    "        \n",
    "        \"\"\"\n",
    "            X is a matrix or 2-D numpy array, represnting testing instances. \n",
    "            Each testing instance is a feature vector. \n",
    "            \n",
    "            Return the predictions of all instances in a list.\n",
    "            \n",
    "            TODO: 6. Revise the following for-loop to call ind_predict to get predictions\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for i in range(inp.shape[0]):\n",
    "            result.append(self.ind_predict(inp[i]))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4vXXHnP-gHD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bg4eRJ58-gHF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MZqOzdjh-gHG"
   },
   "source": [
    "### Test Accruacy should be greater than 0.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nRTbwtIG-gHH",
    "outputId": "b20072b8-597e-477b-f5fb-33c4f01b993c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RandomForest at 0x7f8271f2f8b0>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForest(n_trees= 100, n_split=None)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uoq3tFQ_-gHJ",
    "outputId": "9f200c99-d3fa-47bd-9541-eb98f34189e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict_all(X_test)\n",
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SUwAMpuT-gHK"
   },
   "source": [
    "My data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RandomForest at 0x7fdfacca7e80>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForest(n_trees= 100, n_split=None)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8636668243456322"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = clf.predict_all(X_test)\n",
    "(pred == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DecisionTree&RandomForest.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
